{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715d743b-a738-47e1-b19c-445c106d9209",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21778-91d5-4473-81fd-e9778ff91512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafabric/Mistral7B/mistral-7b-v0.1.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609037c7-ff32-48bf-b9c9-5577802639e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/local\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52ca2d",
   "metadata": {},
   "source": [
    "## Sample script provided by Z by HP documentation to register a `mistral-7b-v0.1.Q4_K_M.gguf` model\n",
    "The model was successfully registered and deployed in MLflow (AIS built-in); however, inference attempts resulted in a \"prediction:null\" output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c8c574-66f5-4ef0-a084-f703fc74c44d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing experiment in MLflow.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8acf57f53de4ff7bc9c9bb380e21b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da5e6fcccb4c969105d9ffa7d2646f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/06 23:55:43 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - tokenizers (current: uninstalled, required: tokenizers==0.20.3)\n",
      " - httpx (current: 0.28.1, required: httpx==0.27.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and artifacts successfully registered in MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Mistral_Chatbot' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model with execution ID: 5731041d9e384961bd1d1c09ba9a1488\n",
      "Model registered successfully. Run ID: 5731041d9e384961bd1d1c09ba9a1488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'Mistral_Chatbot'.\n"
     ]
    }
   ],
   "source": [
    "# register Mistral gguf\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "class AIStudioChatbotService(PythonModel):\n",
    "    @classmethod\n",
    "    def log_model(cls, model_folder=None, demo_folder=\"demo\"):\n",
    "        # Ensure the demo folder exists\n",
    "        if demo_folder and not os.path.exists(demo_folder):\n",
    "            os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "        # Define input schema for the model\n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"string\", \"query\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"document\")\n",
    "        ])\n",
    "        \n",
    "        # Define output schema for the model\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"chunks\"),\n",
    "            ColSpec(\"string\", \"history\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"output\"),\n",
    "            ColSpec(\"boolean\", \"success\")\n",
    "        ])\n",
    "        \n",
    "        # Define parameters schema for additional settings\n",
    "        param_schema = ParamSchema([\n",
    "            ParamSpec(\"add_pdf\", \"boolean\", False),\n",
    "            ParamSpec(\"get_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"set_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"reset_history\", \"boolean\", False)\n",
    "        ])\n",
    "        \n",
    "        # Combine schemas into a model signature\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "\n",
    "        # Define model artifacts\n",
    "        artifacts = {\"demo\": demo_folder}\n",
    "        if model_folder:\n",
    "            artifacts[\"models\"] = model_folder\n",
    "\n",
    "        # Log the model in MLflow\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"aistudio_chatbot_service\",\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=[\n",
    "                \"pyyaml\",\n",
    "                \"tokenizers==0.20.3\",\n",
    "                \"httpx==0.27.2\",\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model and artifacts successfully registered in MLflow.\")\n",
    "\n",
    "# Initialize the MLflow experiment\n",
    "print(\"Initializing experiment in MLflow.\")\n",
    "mlflow.set_experiment(\"AIStudioChatbot_Service\")\n",
    "\n",
    "# Define required paths\n",
    "model_folder = \"/home/jovyan/datafabric/Mistral7B/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "demo_folder = \"demo\"   \n",
    "\n",
    "# Ensure required directories exist before proceeding\n",
    "if demo_folder and not os.path.exists(demo_folder):\n",
    "    os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "# Start an MLflow run and log the model\n",
    "with mlflow.start_run(run_name=\"AIStudioChatbot_Service_Run\") as run:\n",
    "    AIStudioChatbotService.log_model(\n",
    "        demo_folder=demo_folder,\n",
    "        model_folder=model_folder\n",
    "    )\n",
    "    \n",
    "    # Register the model in MLflow\n",
    "    model_uri = f\"runs:/{run.info.run_id}/aistudio_chatbot_service\"\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"Mistral_Chatbot\",\n",
    "    )\n",
    "    print(f\"Registered model with execution ID: {run.info.run_id}\")\n",
    "    print(f\"Model registered successfully. Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af34810",
   "metadata": {},
   "source": [
    "## Modified script to register the gemma-2B model (`gemma-2b-it.Q4_K_M.gguf`) using llama-python-cpp framework on MLflow (AIS built-in)\n",
    "The model was successfully registered in MLflow (AIS built-in), and inference was performed successfully after loading the model from MLflow. However, deployment via Swagger was unsuccessful despite extensive troubleshooting efforts. As a result, we opted to deploy the pre-trained Gemma-2B model directly on the backend of our web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6ae38-9ea4-484f-b13e-d134f68df86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the model registry and score\n",
    "model_uri = f\"models:/{reg_model_name}/1\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "score_model(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba777b3-38df-4ab0-8d40-49e27b158e9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing experiment in MLflow.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f7b3b10426431c9457a1e657628c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c364346ea34a3ab9bd87d0e8656ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and artifacts successfully registered in MLflow.\n",
      "Registered model with execution ID: 32a27d393d8b4c1c94c71b126a7982fb\n",
      "Model registered successfully. Run ID: 32a27d393d8b4c1c94c71b126a7982fb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Gemma_test_0607' already exists. Creating a new version of this model...\n",
      "Created version '4' of model 'Gemma_test_0607'.\n"
     ]
    }
   ],
   "source": [
    "# register Gemma gguf 0607\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, ParamSchema, ParamSpec\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "class AIStudioChatbotService(PythonModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._llama = Llama(\n",
    "            model_path=\"/home/jovyan/local/gemma-2b-it.Q4_K_M.gguf\", #YOUR MODEL PATH HERE\n",
    "            n_ctx=8192,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "        print(f\"Type of prompt is: {type(prompt)}\")\n",
    "        result = self._llama(prompt, temperature=0.0, max_tokens=256, stop=[\"</s>\"])\n",
    "        text = result[\"choices\"][0][\"text\"]\n",
    "        outputs={\n",
    "                \"chunks\": \"\",\n",
    "                \"history\": \"\",\n",
    "                \"prompt\": prompt,\n",
    "                \"output\": text.strip(),\n",
    "                \"success\": True\n",
    "        }\n",
    "        return outputs\n",
    "        \n",
    "    @classmethod\n",
    "    def log_model(cls, model_folder=None, demo_folder=\"demo\"):\n",
    "        # Ensure the demo folder exists\n",
    "        if demo_folder and not os.path.exists(demo_folder):\n",
    "            os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "        # Define input schema for the model\n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"string\", \"query\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"document\")\n",
    "        ])\n",
    "        \n",
    "        # Define output schema for the model\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"chunks\"),\n",
    "            ColSpec(\"string\", \"history\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"output\"),\n",
    "            ColSpec(\"boolean\", \"success\")\n",
    "        ])\n",
    "        \n",
    "        # Define parameters schema for additional settings\n",
    "        param_schema = ParamSchema([\n",
    "            ParamSpec(\"add_pdf\", \"boolean\", False),\n",
    "            ParamSpec(\"get_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"set_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"reset_history\", \"boolean\", False)\n",
    "        ])\n",
    "        \n",
    "        # Combine schemas into a model signature\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "\n",
    "        # Define model artifacts\n",
    "        artifacts = {\"demo\": demo_folder}\n",
    "        if model_folder:\n",
    "            artifacts[\"models\"] = model_folder\n",
    "\n",
    "        # Log the model in MLflow\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"aistudio_chatbot_service\",\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=[\n",
    "                \"pyyaml\",\n",
    "                \"tokenizers==0.20.3\",\n",
    "                \"httpx==0.27.2\",\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model and artifacts successfully registered in MLflow.\")\n",
    "\n",
    "# Initialize the MLflow experiment\n",
    "print(\"Initializing experiment in MLflow.\")\n",
    "mlflow.set_experiment(\"AIStudioChatbot_Service\")\n",
    "\n",
    "# Define required paths\n",
    "model_folder = \"/home/jovyan/local/gemma-2b-it.Q4_K_M.gguf\"\n",
    "demo_folder = \"demo\"   \n",
    "\n",
    "# Ensure required directories exist before proceeding\n",
    "if demo_folder and not os.path.exists(demo_folder):\n",
    "    os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "# Start an MLflow run and log the model\n",
    "with mlflow.start_run(run_name=\"Gemma_Test_Run_gguf_0607\") as run:\n",
    "    AIStudioChatbotService.log_model(\n",
    "        demo_folder=demo_folder,\n",
    "        model_folder=model_folder\n",
    "    )\n",
    "    \n",
    "    # Register the model in MLflow\n",
    "    model_uri = f\"runs:/{run.info.run_id}/aistudio_chatbot_service\"\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"Gemma_test_0607\",\n",
    "    )\n",
    "    print(f\"Registered model with execution ID: {run.info.run_id}\")\n",
    "    print(f\"Model registered successfully. Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46aaf4-7861-41dd-a612-a42d6abfee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/07 21:23:52 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - tokenizers (current: uninstalled, required: tokenizers==0.20.3)\n",
      " - httpx (current: 0.28.1, required: httpx==0.27.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/06/07 21:23:54 WARNING mlflow.pyfunc.model: The underlying model does not support passing additional parameters to the predict function. `params` {'add_pdf': False, 'get_prompt': False, 'set_prompt': False, 'reset_history': False} will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of prompt is: <class 'str'>\n",
      "{'chunks': '', 'history': '', 'prompt': 'What is MLflow?', 'output': \"MLflow is an open-source platform for managing the entire machine learning lifecycle, from data preparation to model deployment. It provides a centralized repository for storing and tracking all the steps of the machine learning process, making it easier to track, reproduce, and improve the model's performance.\\n\\n**Key features of MLflow:**\\n\\n* **Data Management:** MLflow allows you to store and manage data in a central repository, ensuring data quality and version control.\\n* **Model Tracking:** MLflow tracks the entire machine learning process, from data preparation to model deployment, making it easy to track and reproduce the model's performance.\\n* **Collaboration:** MLflow provides a platform for collaboration among data scientists, engineers, and business users, facilitating knowledge sharing and reproducibility.\\n* **Model Versioning:** MLflow allows you to version your models, enabling you to track changes and revert to previous versions if necessary.\\n* **Integration with Various Tools:** MLflow integrates with popular tools and platforms, such as Apache Spark, scikit-learn, and TensorFlow, making it easy to use with existing workflows.\\n\\n**Benefits of using MLflow:**\\n\\n* **Improved Model Reproducibility:** MLflow makes it easier to reproduce the model's results, reducing the risk\", 'success': True}\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the model registry and run inference\n",
    "import pandas as pd\n",
    "\n",
    "def run_model_inference(loaded_model, m_input):\n",
    "    # Use inference to predict output from the customized PyFunc model\n",
    "    scores = loaded_model.predict(m_input)\n",
    "    print(scores)\n",
    "    \n",
    "# Load the model from the model registry and score\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri) # example: model_uri = f\"models:/{reg_model_name}/1\"\n",
    "m_input={'prompt':\"What is MLflow?\", \"query\": \"\", \"document\": \"\"}\n",
    "run_model_inference(loaded_model, m_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8936887-30ec-4083-90a7-3f39eac89419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is MLflow?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check input prompt for the model\n",
    "m_input[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02d29c-33ba-41d3-b01f-7e6beb25b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/07 21:25:20 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r /phoenix/mlflow/172877855724559448/9a853b804d004fc6b242706337371b85/artifacts/aistudio_chatbot_service/requirements.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/phoenix/mlflow/172877855724559448/9a853b804d004fc6b242706337371b85/artifacts/aistudio_chatbot_service/requirements.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model dependencies\n",
    "model_uri = \"runs:/9a853b804d004fc6b242706337371b85/aistudio_chatbot_service\" ## !! REPLACE with your model URI\n",
    "mlflow.pyfunc.get_model_dependencies(model_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
